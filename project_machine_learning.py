# -*- coding: utf-8 -*-
"""Project Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p3GoGhgYQfzjC1p_hDQQ7w3Utc4cfnIN
"""

import os
# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version
# For example:
# spark_version = 'spark-3.0.3'
spark_version = 'spark-3.0.3'
os.environ['SPARK_VERSION']=spark_version

# Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz
!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz
!pip install -q findspark

# Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop2.7"

# Start a SparkSession
import findspark
findspark.init()
# Start Spark session
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("DataFrameBasics").getOrCreate()

import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import math
import plotly.express as px

from sklearn.cluster import KMeans

# Start Spark session
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("DataFrameBasics").getOrCreate()

from google.colab import drive
drive.mount('/gdrive')

import pandas as pd
IMbd_ratings_df = pd.read_csv("/content/IMDb_ratings.csv")
IMbd_ratings_df

import pandas as pd
movies1_df = pd.read_csv("/content/IMDb_movies1.csv")
movies1_df

#IMbd_ratings_df = IMbd_ratings_df.toPandas()
#IMbd_ratings_df

#Selecting relevant data from IMDb_ratings
clean_ratings_df = IMbd_ratings_df[["imdb_title_id",
                             "weighted_average_vote",
                             "total_votes",
                             "allgenders_0age_votes",
                             "allgenders_18age_votes",
                             "allgenders_30age_votes",
                             "allgenders_45age_votes",
                             "males_allages_votes",
                             "females_allages_votes"]]
clean_ratings_df.head()

#renaming headers in ratings data to avoid confusion
new_names = {"allgenders_0age_votes":"tot_voters_below_18",
            "allgenders_18age_votes":"tot_voters_below_30",
            "allgenders_30age_votes":"tot_voters_below_45",
            "allgenders_45age_votes" : "tot_voters_above_45",
            "males_allages_votes":"tot_male_voters",
             "females_allages_votes": "tot_female_voters"}
ratings_df = clean_ratings_df.rename(columns=new_names)
ratings_df.head(5)

imdb_data_df = pd.merge(movies1_df,ratings_df, on="imdb_title_id", how="outer")
imdb_data_df.head()

#total null values in each column
imdb_data_df.isnull().sum()

#Combining under 18 and under 30 columns to reduce null values in tot_voters_below_18 column
below_18 = imdb_data_df["tot_voters_below_18"]
below_30 = imdb_data_df["tot_voters_below_30"]

combined = []

#Initiating for loop to add values in below 18 and below 30 columns 
#Appending it to combined list to replace both columns by combined

for i in range(len(imdb_data_df)):
    if math.isnan(below_18[i]) is True & math.isnan(below_30[i]) is True:
        combined.append(below_18[i] + below_30[i])
        

    elif math.isnan(below_18[i]) is True and math.isnan(below_30[i]) is False:
        combined.append(below_30[i])
            
    elif math.isnan(below_18[i]) == False and math.isnan(below_30[i]) == True:
        combined.append(below_18[i])
            
    else:
        combined.append(0)

# Dropping under 18 columns
imdb_data_df = imdb_data_df.drop("tot_voters_below_18", axis = 1)

# Repacing under 18 and under 30 columns by under 30(combined)
imdb_data_df["tot_voters_below_18"] = combined

#dropping empty values
imdb_data_df = imdb_data_df.dropna()
imdb_data_df

imdb_data_df

Clean_imdb_data_df = imdb_data_df.drop(["imdb_title_id"],1)
Clean_imdb_data_df

Clean_imdb_data_df = imdb_data_df.drop_duplicates()
Clean_imdb_data_df

Clean_imdb_data_df.dtypes

print(f"Duplicates entries: {Clean_imdb_data_df.duplicated().sum()}")

Clean_imdb_data_df1 = imdb_data_df.drop(["imdb_title_id","budget"],1)
Clean_imdb_data_df1

Clean_imdb_data_df1 = Clean_imdb_data_df1.dropna()
Clean_imdb_data_df1

#Clean_imdb_data_df1.groupby("language")

import matplotlib.pyplot as plt
duration = Clean_imdb_data_df1 ['duration'].tolist()
avg_vote = Clean_imdb_data_df1 ['avg_vote'].tolist()
plt.scatter(duration,avg_vote)
plt.title('duration vs avg_vote')
plt.xlabel('duration')
plt.ylabel('avg_vote')
plt.show()

import matplotlib.pyplot as plt
reviews_from_users = Clean_imdb_data_df1 ['reviews_from_users'].tolist()
weighted_average_vote = Clean_imdb_data_df1 ['weighted_average_vote'].tolist()
plt.scatter(reviews_from_users,weighted_average_vote)
plt.title('	reviews_from_users vs weighted_average_vote')
plt.xlabel('reviews_from_users')
plt.ylabel('weighted_average_vote')
plt.show()

import matplotlib.pyplot as plt
duration = Clean_imdb_data_df1 ['duration'].tolist()
weighted_average_vote = Clean_imdb_data_df1 ['weighted_average_vote'].tolist()
plt.scatter(reviews_from_users,weighted_average_vote)
plt.title('	duration vs weighted_average_vote')
plt.xlabel('duration')
plt.ylabel('weighted_average_vote')
plt.show()

# Determine the number of unique values in each column.
ratings_cat = Clean_imdb_data_df1.dtypes[Clean_imdb_data_df1.dtypes == "object"].index.tolist()
ratings_cat

Clean_imdb_data_df1[ratings_cat ].nunique()

# Look at duration value counts for binning
genre1 = Clean_imdb_data_df1.genre1.value_counts()
genre1

# Visualize the value counts of genre
genre1.plot.density()

# Determine which values to replace if counts are less than ...?
replace_genre1 = list(genre1 [genre1 < 500].index)

# Replace in dataframe
for app in replace_genre1:
      Clean_imdb_data_df1.genre1 = Clean_imdb_data_df1.genre1.replace(app,"Other")
    
# Check to make sure binning was successful
Clean_imdb_data_df1.genre1.value_counts()

# Look at duration value counts for binning
duration = Clean_imdb_data_df1.duration.value_counts()
duration

# Visualize the value counts of duration
duration.plot.density()

# Determine which values to replace if counts are less than ...?
replace_duration = list(duration [duration < 500].index)

# Replace in dataframe
for app in replace_duration:
      Clean_imdb_data_df1.duration = Clean_imdb_data_df1.duration.replace(app,"Other")
    
# Check to make sure binning was successful
Clean_imdb_data_df.duration.value_counts()

country = Clean_imdb_data_df1.country.value_counts()
country

# Visualize the value counts of duration
country.plot.density()

# Determine which values to replace if counts are less than ...?
replace_country = list(country [country < 500].index)

# Replace in dataframe
for app in replace_country:
      Clean_imdb_data_df1.country = Clean_imdb_data_df1.country.replace(app,"Other")
    
# Check to make sure binning was successful
Clean_imdb_data_df1.country.value_counts()

from sklearn.preprocessing import StandardScaler,OneHotEncoder
# Create a OneHotEncoder instance
enc = OneHotEncoder(sparse=False)
# Fit and transform the OneHotEncoder using the categorical variable list
encode_df = pd.DataFrame(enc.fit_transform(Clean_imdb_data_df1[ratings_cat]))
# Add the encoded variable names to the dataframe
encode_df.columns = enc.get_feature_names(ratings_cat)
encode_df.head()

# Merge one-hot encoded features and drop the originals
Clean_imdb_data_df1 = Clean_imdb_data_df1.merge(encode_df,left_index=True, right_index=True)
Clean_imdb_data_df1 = Clean_imdb_data_df1.drop(ratings_cat,1)
Clean_imdb_data_df1.head()

Clean_imdb_data_df1

Clean_imdb_data_df1 = pd.get_dummies(Clean_imdb_data_df1,columns =['duration'] )
Clean_imdb_data_df1

ModifiedClean_imdb_data_df1 = Clean_imdb_data_df1[Clean_imdb_data_df1["weighted_average_vote"]>1]
ModifiedClean_imdb_data_df1

# Standardize the data with StandardScaler()
Clean_imdb_data_df1_scaled = StandardScaler().fit_transform(Clean_imdb_data_df1)
print(Clean_imdb_data_df1_scaled [0:5])

from sklearn.decomposition import PCA
# Initialize PCA model
pca = PCA(n_components=3)

# Get two principal components for the iris data.
Clean_imdb_data_df1_pca = pca.fit_transform(Clean_imdb_data_df1_scaled)
Clean_imdb_data_df1_pca

# Create a DataFrame with the three principal components.
Clean_imdb_data_df1_pca = pd.DataFrame(
    data = Clean_imdb_data_df1_pca, columns = ["PC 1", "PC 2", "PC 3"]
)
Clean_imdb_data_df1_pca

pca.explained_variance_ratio_

# Initialize the K-Means model.
model = KMeans(n_clusters=10, random_state=1)
# Fit the model
model.fit(Clean_imdb_data_df1_pca)
# Predict clusters
predictions = model.predict(Clean_imdb_data_df1_pca)
print(predictions)
Clean_imdb_data_df1_pca["Class"] = model.labels_

# Create a new DataFrame including predicted clusters and cryptocurrencies features.
Clean_imdb_data_df1_pca["Class"] = model.labels_
Clean_imdb_data_df1_pca.head()

Clean_imdb_data_df1_pca["Class"].value_counts()

#Joining dataFrame through "join" function
clustered_df = Clean_imdb_data_df.join(Clean_imdb_data_df1_pca, how='inner')
clustered_df.head(10)

# Print the shape of the clustered_df
print(clustered_df.shape)
clustered_df.head(5)

import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

plt.scatter(clustered_df.duration, clustered_df.weighted_average_vote)
plt.xlabel('duration')
plt.ylabel('weighted_average_vote')
plt.show()

X = clustered_df.duration.values.reshape(-1, 1)

X[:5]

#When we examine the shape of X, we see that there are 693 rows and 1 column:
X.shape

y = clustered_df.weighted_average_vote

model = LinearRegression()

model.fit(X, y)

y_pred = model.predict(X)
print(y_pred.shape)

plt.scatter(X, y)
plt.plot(X, y_pred, color='red')
plt.show()

print(model.coef_)
print(model.intercept_)

clustered_df_encoded = pd.get_dummies(clustered_df, columns=["genre1","imdb_title_id","country","language 1","budget",])
clustered_df_encoded

y = clustered_df_encoded["weighted_average_vote"]
X = clustered_df_encoded.drop(columns="weighted_average_vote")

#from collections import Counter
#from sklearn.model_selection import train_test_split
#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
#Counter(y_train)

##from imblearn.over_sampling import RandomOverSampler
#ros = RandomOverSampler(random_state=1)
#X_resampled, y_resampled = ros.fit_resample(X_train, y_train)

#Counter(y_resampled)

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

y

#Preprocessing the Data for a Neural Network
# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

print(X_train)

# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
number_input_features = len(X_train.columns)
hidden_nodes_layer1 = 5000
hidden_nodes_layer2 = 3000
hidden_nodes_layer3 = 1000


nn = tf.keras.models.Sequential()

# First hidden layer
nn.add(
    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation="relu")
)

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation="relu"))

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation="relu"))


# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model
nn.summary()

# Compile the model
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
new_fit_model = nn.fit(X_train_scaled, y_train, epochs=100, shuffle=True)

# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import pandas as pd
import tensorflow as tf

y = clustered_df_encoded["weighted_average_vote"]
X = clustered_df_encoded.drop(columns="weighted_average_vote")

# Split training/test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create a StandardScaler instance
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create the SVM model
svm = SVC(kernel='linear')

# Train the model
#svm.fit(X_train, y_train)

from sklearn import preprocessing
from sklearn import utils

lab_enc = preprocessing.LabelEncoder()
encoded = lab_enc.fit_transform( y_train)

print(utils.multiclass.type_of_target(y_train))
print(utils.multiclass.type_of_target(y_train.astype('int')))
print(utils.multiclass.type_of_target(encoded))

clf = LogisticRegression()
clf.fit(X_train,encoded)
print("LogisticRegression")
#print(clf.predict(prediction_data_test))

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,
   y,  random_state=1)
X_train.shape

#Scale and Normalize Data
from sklearn.preprocessing import StandardScaler
data_scaler = StandardScaler()

Imbd_data_scaled = data_scaler.fit_transform(clustered_df_encoded)

Imbd_data_scaled[:5]

import numpy as np
print(np.mean(Imbd_data_scaled[:,0]))
print(np.std(Imbd_data_scaled[:,0]))

model.fit(X_train, y_train)

from sklearn.svm import SVC
model = SVC(kernel='linear')
model.fit(X_train, y_train)
lab_enc = preprocessing.LabelEncoder()
y_train_encoded = lab_enc.fit_transform(y_train)
print(y_train_encoded)
print(utils.multiclass.type_of_target(y_train))
print(utils.multiclass.type_of_target(y_train.astype('int')))
print(utils.multiclass.type_of_target(y_train_encoded))

y_pred = model.predict(X_test)
results = pd.DataFrame({
   "Prediction": y_pred,
   "Actual": y_test
}).reset_index(drop=True)
results.head()



from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,
   y, random_state=1)

y[:5]

# Splitting into Train and Test sets.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78,train_size=0.80)

# Determine the shape of our training and testing sets.
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

# Creating a StandardScaler instance.
scaler = StandardScaler()
# Fitting the Standard Scaler with the training data.
X_scaler = scaler.fit(X_train)

# Scaling the data.
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

from sklearn.linear_model import LogisticRegression
import numpy as np
classifier = LogisticRegression(solver='lbfgs',
  max_iter=200,
  random_state=1)

X_variables = np.array["weighted_average_vote"]
y_variables = np.array["total_votes"]
u_variables = np.array([i])
lr = LogisticRegression()
lr.fit(x_variables, y_variables)
lr.fit(x_variables, y_variables)
for i in range(20):
   print("x=" + str(i) + " --> " + str(lr.predict_proba([[i]])))

lab_enc = preprocessing.LabelEncoder()
clustered_df_encoded = lab_enc.fit_transform(y_train)
print(training_scores_encoded)
print(utils.multiclass.type_of_target(y_train)))
print(utils.multiclass.type_of_target(y_train).astype('int')))
print(utils.multiclass.type_of_target(clustered_df_encoded))